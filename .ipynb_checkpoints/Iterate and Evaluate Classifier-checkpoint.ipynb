{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Naive Bayes Sentiment Analysis Challenge</H1><br><br>\n",
    "I really want to overfit this time.<br><br>\n",
    "Sentiment raw data was taken from the <a href='https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences'>UCI Machine Learning Repository.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Class Imbalance</H2><br><br>\n",
    "It turns out that all three data sets have exacty 1000 points, 500 positive and 500 negative. There is no class imbalance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('amazon_cells_labelled.txt', engine='python', header=None, sep=None)\n",
    "data.columns = ['text', 'sentiment']\n",
    "data['positive_sentiment'] = np.where((data.sentiment == 1), True, False)\n",
    "\n",
    "def get_words(series):\n",
    "    words = []\n",
    "    for item in series: \n",
    "        words += item #put all words in same list\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    for i, word in enumerate(words):\n",
    "        word = word.translate(translator)\n",
    "        words[i] = word #strip away punctutation\n",
    "    return words\n",
    "\n",
    "#This is the confusion matrix calculator I wrote for the evaluation drill...\n",
    "def conf_calc(predictions, actual):\n",
    "    true_positives = np.where((np.where((actual == True), True, None) == predictions), True, False)\n",
    "    false_positives = np.where((np.where((actual == False), True, None) == predictions), True, False)\n",
    "    true_negatives = np.where((np.where((actual == False), False, None) == predictions), True, False)\n",
    "    false_negatives = np.where((np.where((actual == True), False, None) == predictions), True, False)\n",
    "    num_true_positives = true_positives.sum()\n",
    "    total_positives = data.positive_sentiment.sum()\n",
    "    num_true_negatives = true_negatives.sum()\n",
    "    total_negatives = len(data) - total_positives\n",
    "\n",
    "    print('sensitivity is: {}% (positives correctly identified)'.format(num_true_positives/total_positives)) \n",
    "    print('specificity is: {}% (negatives correctly identified)'.format(num_true_negatives/total_negatives))\n",
    "    conf_matrix = pd.DataFrame(index=['actual_false', 'actual_true'], columns=['predicted false', 'predicted true'])\n",
    "    conf_matrix.loc['actual_true'] = [false_positives.sum(), num_true_positives]\n",
    "    conf_matrix.loc['actual_false'] = [num_true_negatives, false_negatives.sum()]\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_words = pd.Series(get_words(data[data['sentiment'] == 1].text.str.lower().str.split()))\n",
    "negative_words = pd.Series(get_words(data[data['sentiment'] == 0].text.str.lower().str.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg = list(negative_words.value_counts().index)\n",
    "pos = list(positive_words.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = neg + pos #simply grab all words for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in keywords:\n",
    "    re_string = '[^a-zA-Z]' + key + '[^a-zA-Z]' #to match words with spaces or punctuations around only\n",
    "    data[str(key)] = data.text.apply(lambda x: bool(re.search(re_string, str(x), re.IGNORECASE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overfit = bnb.fit(data[keywords], data.positive_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     747\n",
       "False    253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((overfit.predict(data[keywords]) == data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 1.0\n",
      "specificity is: 0.494\n",
      "              predicted false  predicted true\n",
      "actual_false              247               0\n",
      "actual_true               253             500\n"
     ]
    }
   ],
   "source": [
    "conf_calc(overfit.predict(data[keywords]), data.positive_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still only have 77% accuracy on the training data...<br>\n",
    "Let's try training on one half of the data set and running it against the other half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holdout = BernoulliNB().fit(data.iloc[:500][keywords], data.positive_sentiment.iloc[:500])\n",
    "#the distribution of positive and negative sentiments among our points is even, so splitting it like this\n",
    "#still avoids class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     251\n",
       "False    249\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((holdout.predict(data.iloc[500:][keywords]) == data.iloc[500:].positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 0.478\n",
      "specificity is: 0.024\n",
      "              predicted false  predicted true\n",
      "actual_false               12               0\n",
      "actual_true               249             239\n"
     ]
    }
   ],
   "source": [
    "conf_calc(holdout.predict(data.iloc[500:][keywords]), data.iloc[500:].positive_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     314\n",
       "False    186\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((holdout.predict(data.iloc[:500][keywords]) == data.iloc[:500].positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 0.522\n",
      "specificity is: 0.224\n",
      "              predicted false  predicted true\n",
      "actual_false              112               0\n",
      "actual_true               127             261\n"
     ]
    }
   ],
   "source": [
    "conf_calc(overfit.predict(data.iloc[:500][keywords]), data.iloc[:500].positive_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     565\n",
       "False    435\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((holdout.predict(data[keywords]) == data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 1.0\n",
      "specificity is: 0.13\n",
      "              predicted false  predicted true\n",
      "actual_false               65               0\n",
      "actual_true               435             500\n"
     ]
    }
   ],
   "source": [
    "conf_calc(holdout.predict(data[keywords]), data.positive_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see about 64% accuracy with the training group. With the holdout group, we see about 50% accuracy, which is as good as guessing all positive or all negative. <B>This is because there is no class imbalance here: all three data sets have exactly 500 positive and 500 negative points.</B> This looks like overfitting to me.<br>\n",
    "Let's compare these two models against the other data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_data = pd.read_csv('imdb_labelled.txt', engine='python', header=None, sep='\\t', quoting=3)\n",
    "imdb_data.columns = ['text', 'sentiment']\n",
    "imdb_data['positive_sentiment'] = np.where((imdb_data.sentiment == 1), True, False)\n",
    "for key in keywords:\n",
    "    re_string = '[^a-zA-Z]' + key + '[^a-zA-Z]' #to match words with spaces or punctuations around only\n",
    "    imdb_data[str(key)] = imdb_data.text.apply(lambda x: bool(re.search(re_string, str(x), re.IGNORECASE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     606\n",
       "False    394\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((overfit.predict(imdb_data[keywords]) == imdb_data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts()  #'overfit' model against imdb set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     523\n",
       "False    477\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((holdout.predict(imdb_data[keywords]) == imdb_data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts() #'holdout' model against imdb set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_data = pd.read_csv('yelp_labelled.txt', engine='python', header=None, sep='\\t', quoting=3)\n",
    "yelp_data.columns = ['text', 'sentiment']\n",
    "yelp_data['positive_sentiment'] = np.where((yelp_data.sentiment == 1), True, False)\n",
    "for key in keywords:\n",
    "    re_string = '[^a-zA-Z]' + key + '[^a-zA-Z]' #to match words with spaces or punctuations around only\n",
    "    yelp_data[str(key)] = yelp_data.text.apply(lambda x: bool(re.search(re_string, str(x), re.IGNORECASE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     569\n",
       "False    431\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((overfit.predict(yelp_data[keywords]) == yelp_data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts() #'overfit' model against yelp set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     513\n",
       "False    487\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((holdout.predict(yelp_data[keywords]) == yelp_data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts() #'holdout' model against yelp set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Did we overfit?</H2><br><br>\n",
    "I'd say we did! Our 'overfit' and 'holdout' models both give a 50%-60% performance against the other sets, and the holdout test suggests overfitting even within our training set. I'd say that using every single word as a feature is a pretty lousy strategy overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
