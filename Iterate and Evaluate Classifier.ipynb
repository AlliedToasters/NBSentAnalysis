{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Naive Bayes Sentiment Analysis Challenge</H1><br><br>\n",
    "I really want to overfit this time.<br><br>\n",
    "Sentiment raw data was taken from the <a href='https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences'>UCI Machine Learning Repository.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Class Imbalance</H2><br><br>\n",
    "It turns out that all three data sets have exacty 1000 points, 500 positive and 500 negative. There is no class imbalance here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>My Overfitting Model Design: Use Every Word as a Feature</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('amazon_cells_labelled.txt', engine='python', header=None, sep=None)\n",
    "data.columns = ['text_', 'sentiment']\n",
    "data['positive_sentiment'] = np.where((data.sentiment == 1), True, False)\n",
    "\n",
    "def get_words(series):\n",
    "    words = []\n",
    "    for item in series: \n",
    "        words += item #put all words in same list\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    for i, word in enumerate(words):\n",
    "        word = word.translate(translator)\n",
    "        words[i] = word #strip away punctutation\n",
    "    return words\n",
    "\n",
    "#This is the confusion matrix calculator I wrote for the evaluation drill...\n",
    "def conf_calc(predictions, actual):\n",
    "    true_positives = np.where((np.where((actual == True), True, None) == predictions), True, False)\n",
    "    false_positives = np.where((np.where((actual == False), True, None) == predictions), True, False)\n",
    "    true_negatives = np.where((np.where((actual == False), False, None) == predictions), True, False)\n",
    "    false_negatives = np.where((np.where((actual == True), False, None) == predictions), True, False)\n",
    "    num_true_positives = true_positives.sum()\n",
    "    total_positives = actual.sum()\n",
    "    num_true_negatives = true_negatives.sum()\n",
    "    total_negatives = len(actual) - total_positives\n",
    "\n",
    "    print('sensitivity is: {}% (positives correctly identified)'.format(str(100*num_true_positives/total_positives)[:4])) \n",
    "    print('specificity is: {}% (negatives correctly identified)'.format(str(100*num_true_negatives/total_negatives)[:4]))\n",
    "    conf_matrix = pd.DataFrame(index=['actual_false', 'actual_true'], columns=['predicted false', 'predicted true'])\n",
    "    conf_matrix.loc['actual_true'] = [false_negatives.sum(), num_true_positives]\n",
    "    conf_matrix.loc['actual_false'] = [num_true_negatives, false_positives.sum()]\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 50.0% (positives correctly identified)\n",
      "specificity is: 50.0% (negatives correctly identified)\n",
      "              predicted false  predicted true\n",
      "actual_false                1               1\n",
      "actual_true                 1               1\n"
     ]
    }
   ],
   "source": [
    "fake_act = pd.Series([True, True, False, False])\n",
    "fake_pred = pd.Series([True, False, False, True])\n",
    "conf_calc(fake_act, fake_pred) #testing our evaluator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_words = pd.Series(get_words(data[data['sentiment'] == 1].text_.str.lower().str.split()))\n",
    "negative_words = pd.Series(get_words(data[data['sentiment'] == 0].text_.str.lower().str.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg = list(negative_words.value_counts().index)\n",
    "pos = list(positive_words.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = neg + pos #simply grab all words for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keywords:\n",
    "    re_string = '[^a-zA-Z]' + key + '[^a-zA-Z]' #to match words with spaces or punctuations around only\n",
    "    data[str(key)] = data.text_.apply(lambda x: bool(re.search(re_string, str(x), re.IGNORECASE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overfit = bnb.fit(data[keywords], data.positive_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     903\n",
       "False     97\n",
       "dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((overfit.predict(data[keywords]) == data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>90% Accuracy! HOORAY!!!</H3> <br>\n",
    "The first time around, I made a silly, but highly consequential, mistake: naming the review content column \"text\". It turns out that \"text\" was also a keyword, and thus my script would overwrite this column and begin matching keywords to a column fool of boolean values. WOOPS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 93.8% (positives correctly identified)\n",
      "specificity is: 86.8% (negatives correctly identified)\n",
      "              predicted false  predicted true\n",
      "actual_false              434              66\n",
      "actual_true                31             469\n"
     ]
    }
   ],
   "source": [
    "conf_calc(overfit.predict(data[keywords]), data.positive_sentiment) #Remember, \"True\" is positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try training with a holdout group, on the first 500 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holdout = BernoulliNB().fit(data.iloc[:500][keywords], data.positive_sentiment.iloc[:500])\n",
    "#the distribution of positive and negative sentiments among our points is even, so splitting it like this\n",
    "#still avoids class imbalance problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And running our model on the holdout set gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     326\n",
       "False    174\n",
       "dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((holdout.predict(data.iloc[500:][keywords]) == data.iloc[500:].positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 74.8% (positives correctly identified)\n",
      "specificity is: 56.3% (negatives correctly identified)\n",
      "              predicted false  predicted true\n",
      "actual_false              147             114\n",
      "actual_true                60             179\n"
     ]
    }
   ],
   "source": [
    "conf_calc(holdout.predict(data.iloc[500:][keywords]), data.iloc[500:].positive_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MUCH worse performance than on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     428\n",
       "False     72\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((holdout.predict(data.iloc[:500][keywords]) == data.iloc[:500].positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 94.2% (positives correctly identified)\n",
      "specificity is: 87.0% (negatives correctly identified)\n",
      "              predicted false  predicted true\n",
      "actual_false              208              31\n",
      "actual_true                15             246\n"
     ]
    }
   ],
   "source": [
    "conf_calc(overfit.predict(data.iloc[:500][keywords]), data.iloc[:500].positive_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like textbook overfitting! Let's run the holdout model against the entire data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     754\n",
       "False    246\n",
       "dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((holdout.predict(data[keywords]) == data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 86.8% (positives correctly identified)\n",
      "specificity is: 64.0% (negatives correctly identified)\n",
      "              predicted false  predicted true\n",
      "actual_false              320             180\n",
      "actual_true                66             434\n"
     ]
    }
   ],
   "source": [
    "conf_calc(holdout.predict(data[keywords]), data.positive_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much worse performance than the \"overfit\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><H3>Why The Tendency Towards Positive Sentiment Prediction?</H3><br>\n",
    "You may have noticed that both of these models tend to predict more positive than negative, yielding a higher sensitivity to positive sentiments and a lower specificity to negative sentiments. Why is that? It could have something to do with the length of reviews; maybe negative reviews are long and positives are short, and thus the model interprets the reviews with less words (less features) as positive and only the longest (more features) as negative. Let's take a look:<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_word_count</th>\n",
       "      <th>neg_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.914000</td>\n",
       "      <td>10.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.785772</td>\n",
       "      <td>6.578028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pos_word_count  neg_word_count\n",
       "count      500.000000      500.000000\n",
       "mean         9.914000       10.578000\n",
       "std          6.785772        6.578028\n",
       "min          1.000000        1.000000\n",
       "25%          4.000000        5.000000\n",
       "50%          8.000000       10.000000\n",
       "75%         14.000000       15.000000\n",
       "max         30.000000       30.000000"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = pd.Series(data[data['sentiment'] == 1].text_.str.lower().str.split())\n",
    "negative_words = pd.Series(data[data['sentiment'] == 0].text_.str.lower().str.split())\n",
    "data['pos_word_count'] = positive_words.apply(lambda x: len(x))\n",
    "data['neg_word_count'] = negative_words.apply(lambda x: len(x))\n",
    "data[['pos_word_count', 'neg_word_count']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, negative reviews tend to be longer, so this could explain our model's tendancy to usually predict negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> My Models Vs. Other Data Sets </H2><br>\n",
    "Let's compare these two models against the other data sets.<br><br>\n",
    "<H3> imdb Data </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_data = pd.read_csv('imdb_labelled.txt', engine='python', header=None, sep='\\t', quoting=3)\n",
    "imdb_data.columns = ['text_', 'sentiment']\n",
    "imdb_data['positive_sentiment'] = np.where((imdb_data.sentiment == 1), True, False)\n",
    "for key in keywords:\n",
    "    re_string = '[^a-zA-Z]' + key + '[^a-zA-Z]' #to match words with spaces or punctuations around only\n",
    "    imdb_data[str(key)] = imdb_data.text_.apply(lambda x: bool(re.search(re_string, str(x), re.IGNORECASE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     640\n",
       "False    360\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((overfit.predict(imdb_data[keywords]) == imdb_data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts()  #'overfit' model against imdb set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 59.8% (positives correctly identified)\n",
      "specificity is: 68.2% (negatives correctly identified)\n",
      "              predicted false  predicted true\n",
      "actual_false              341             159\n",
      "actual_true               201             299\n"
     ]
    }
   ],
   "source": [
    "conf_calc(overfit.predict(imdb_data[keywords]), imdb_data.positive_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     584\n",
       "False    416\n",
       "dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((holdout.predict(imdb_data[keywords]) == imdb_data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts() #'holdout' model against imdb set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 63.6% (positives correctly identified)\n",
      "specificity is: 53.2% (negatives correctly identified)\n",
      "              predicted false  predicted true\n",
      "actual_false              266             234\n",
      "actual_true               182             318\n"
     ]
    }
   ],
   "source": [
    "conf_calc(holdout.predict(imdb_data[keywords]), imdb_data.positive_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models are really lousy, and this is not a surprise. Both overfit our training data. Notice the \"overfit\" model tends to guess negative. Maybe negative imdb reviews are shorter than positive ones?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_word_count</th>\n",
       "      <th>neg_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.128000</td>\n",
       "      <td>13.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.102859</td>\n",
       "      <td>9.036293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>71.000000</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pos_word_count  neg_word_count\n",
       "count      500.000000      500.000000\n",
       "mean        15.128000       13.582000\n",
       "std         10.102859        9.036293\n",
       "min          1.000000        1.000000\n",
       "25%          8.000000        7.000000\n",
       "50%         13.000000       11.000000\n",
       "75%         20.000000       19.000000\n",
       "max         71.000000       56.000000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words_imdb = pd.Series(imdb_data[imdb_data['sentiment'] == 1].text_.str.lower().str.split())\n",
    "negative_words_imdb = pd.Series(imdb_data[imdb_data['sentiment'] == 0].text_.str.lower().str.split())\n",
    "imdb_data['pos_word_count'] = positive_words_imdb.apply(lambda x: len(x))\n",
    "imdb_data['neg_word_count'] = negative_words_imdb.apply(lambda x: len(x))\n",
    "imdb_data[['pos_word_count', 'neg_word_count']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, they are! This supports the idea that just having more words tends to make the models assume the review is negative. The \"holdout\" model, on the other hand, doesn't exhibit the same behavior, so I could be wrong. Note that the \"overfit\" model performs better overall (though still not great.)\n",
    "<br><H3> Yelp Data </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_data = pd.read_csv('yelp_labelled.txt', engine='python', header=None, sep='\\t', quoting=3)\n",
    "yelp_data.columns = ['text_', 'sentiment']\n",
    "yelp_data['positive_sentiment'] = np.where((yelp_data.sentiment == 1), True, False)\n",
    "for key in keywords:\n",
    "    re_string = '[^a-zA-Z]' + key + '[^a-zA-Z]' #to match words with spaces or punctuations around only\n",
    "    yelp_data[str(key)] = yelp_data.text_.apply(lambda x: bool(re.search(re_string, str(x), re.IGNORECASE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     710\n",
       "False    290\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((overfit.predict(yelp_data[keywords]) == yelp_data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts() #'overfit' model against yelp set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 66.4% (positives correctly identified)\n",
      "specificity is: 75.6% (negatives correctly identified)\n",
      "              predicted false  predicted true\n",
      "actual_false              378             122\n",
      "actual_true               168             332\n"
     ]
    }
   ],
   "source": [
    "conf_calc(overfit.predict(yelp_data[keywords]), yelp_data.positive_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     659\n",
       "False    341\n",
       "dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.where((holdout.predict(yelp_data[keywords]) == yelp_data.positive_sentiment), True, False)\n",
    "pd.Series(correct_predictions).value_counts() #'holdout' model against yelp set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity is: 72.8% (positives correctly identified)\n",
      "specificity is: 59.0% (negatives correctly identified)\n",
      "              predicted false  predicted true\n",
      "actual_false              295             205\n",
      "actual_true               136             364\n"
     ]
    }
   ],
   "source": [
    "conf_calc(holdout.predict(yelp_data[keywords]), yelp_data.positive_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our models seem to fare a little better with the yelp data. Again, the \"overfit\" model tends to predict more negative than the \"holdout\" model, and the \"overfit\" has a little better overall accuracy. Just for the sake of curiosity, let's look at the mean review length for Yelp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_word_count</th>\n",
       "      <th>neg_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>10.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.310368</td>\n",
       "      <td>6.212619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pos_word_count  neg_word_count\n",
       "count      500.000000      500.000000\n",
       "mean        11.030000       10.758000\n",
       "std          6.310368        6.212619\n",
       "min          2.000000        1.000000\n",
       "25%          6.000000        5.000000\n",
       "50%         10.000000       10.000000\n",
       "75%         15.000000       15.000000\n",
       "max         32.000000       28.000000"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words_yelp = pd.Series(yelp_data[imdb_data['sentiment'] == 1].text_.str.lower().str.split())\n",
    "negative_words_yelp = pd.Series(yelp_data[imdb_data['sentiment'] == 0].text_.str.lower().str.split())\n",
    "yelp_data['pos_word_count'] = positive_words_yelp.apply(lambda x: len(x))\n",
    "yelp_data['neg_word_count'] = negative_words_yelp.apply(lambda x: len(x))\n",
    "yelp_data[['pos_word_count', 'neg_word_count']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of review lengths for Yelp is almost identical between positive and negative sentiments. This suggests that maybe the tendency for the \"overfit\" model to guess negative has to do with something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Did we overfit?</H2><br><br>\n",
    "I'd say we did! Our 'overfit' and 'holdout' models both give no better than 75% accuracy performance against the other sets, and the holdout test suggests overfitting even within our training set. I'd say that using every single word as a feature is a pretty lousy strategy overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Conclusion</H2><br>\n",
    "I came up with two models that were pretty clearly overfit to the training data, although I am still proud of getting above 90% predictive accuracy on the training set.<br><br>\n",
    "In the process of evaluating these two models, we see that the model named \"overfit\" seems to predict more negative reviews in general, while the \"holdout\" model seems a bit move even-keeled. In a real-life application, these details would have important performance impacts for models.<br><br>\n",
    "While using every single word as a feature was an interesting excersize, it's clearly a bad technique for creating a generally-applicable sentiment analysis model. Trying to hand-pick certain one-word features and identifying meaningful n-grams to be used as features would be an excellent avenue to improve overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
